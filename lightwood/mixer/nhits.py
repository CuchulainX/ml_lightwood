from itertools import product
from typing import Dict, Union

import numpy as np
import pandas as pd
import pytorch_lightning as pl
from pytorch_lightning.callbacks import EarlyStopping

from neuralforecast.models.nhits.nhits import NHITS
from neuralforecast.experiments.utils import create_datasets
from neuralforecast.data.tsloader import TimeSeriesLoader

from lightwood.helpers.log import log
from lightwood.mixer.base import BaseMixer
from lightwood.api.types import PredictionArguments
from lightwood.helpers.general import get_group_matches
from lightwood.data.encoded_ds import EncodedDs, ConcatedEncodedDs


class NHitsMixer(BaseMixer):
    horizon: int
    target: str
    supports_proba: bool
    model_path: str
    hyperparam_search: bool

    default_config: dict = {
        'model': 'n-hits',
        'mode': 'simple',
        'activation': 'SELU',

        'n_time_in': 24 * 3,
        'n_time_out': 24,
        'n_x_hidden': 8,
        'n_s_hidden': 0,

        'stack_types': ['identity', 'identity', 'identity'],
        'constant_n_blocks': 1,
        'constant_n_layers': 2,
        'constant_n_mlp_units': 256,
        'n_pool_kernel_size': [4, 2, 1],
        'n_freq_downsample': [24, 12, 1],
        'pooling_mode': 'max',
        'interpolation_mode': 'linear',
        'shared_weights': False,

        # Optimization and regularization parameters
        'initialization': 'lecun_normal',
        'learning_rate': 0.001,
        'batch_size': 1,
        'n_windows': 32,
        'lr_decay': 0.5,
        'lr_decay_step_size': 2,
        'max_epochs': 1,
        'max_steps': None,
        'early_stop_patience': 20,
        'eval_freq': 500,
        'batch_normalization': False,
        'dropout_prob_theta': 0.0,
        'dropout_prob_exogenous': 0.0,
        'weight_decay': 0,
        'loss_train': 'MAE',
        'loss_hypar': 0.5,
        'loss_valid': 'MAE',
        'random_seed': 1,

        # Data Parameters
        'idx_to_sample_freq': 1,
        'val_idx_to_sample_freq': 1,
        'n_val_weeks': 52,
        'normalizer_y': None,
        'normalizer_x': 'median',
        'complete_windows': False,
        'frequency': 'H',
    }

    def __init__(
            self,
            stop_after: float,
            target: str,
            horizon: int,
            ts_analysis: Dict,
    ):
        """
        Mixer description here.
        
        :param stop_after: time budget in seconds.
        :param target: column to forecast.
        :param horizon: length of forecasted horizon.
        :param ts_analysis: dictionary with miscellaneous time series info, as generated by 'lightwood.data.timeseries_analyzer'.
        """  # noqa
        super().__init__(stop_after)
        self.stable = False
        self.prepared = False
        self.supports_proba = False
        self.target = target
        self.config = NHitsMixer.default_config.copy()

        self.ts_analysis = ts_analysis
        self.horizon = horizon
        self.grouped_by = ['__default'] if not ts_analysis['tss'].group_by else ts_analysis['tss'].group_by
        self.model = None

    def fit(self, train_data: EncodedDs, dev_data: EncodedDs) -> None:
        """
        Fits the N-HITS model.
        """  # noqa
        log.info('Started fitting N-HITS forecasting model')

        cat_ds = ConcatedEncodedDs([train_data, dev_data])
        df = cat_ds.data_frame.sort_values(by=f'__mdb_original_{self.ts_analysis["tss"].order_by[0]}')
        data = {'data': df[self.target],
                'group_info': {gcol: df[gcol].tolist()
                               for gcol in self.grouped_by} if self.ts_analysis['tss'].group_by else {}}

        # 0. figure out why now it is so slow
        # 1. add to json ai right signature (params etc.)
        # 2. adapt data into the expected DFs
        # 3. merge new config into default (copied from instance)
        # 4. train etc.


        # start reference
        train_dataset, val_dataset, test_dataset, scaler_y = create_datasets(mc=self.config,
                                                                             S_df=S_df, Y_df=Y_df, X_df=X_df,
                                                                             f_cols=['Exogenous1', 'Exogenous2'],
                                                                             ds_in_val=294 * 24,
                                                                             ds_in_test=728 * 24)

        train_loader = TimeSeriesLoader(dataset=train_dataset,
                                        batch_size=int(self.config['batch_size']),
                                        n_windows=self.config['n_windows'],
                                        shuffle=True)

        val_loader = TimeSeriesLoader(dataset=val_dataset,
                                      batch_size=int(self.config['batch_size']),
                                      shuffle=False)

        test_loader = TimeSeriesLoader(dataset=test_dataset,
                                       batch_size=int(self.config['batch_size']),
                                       shuffle=False)

        self.config['n_x'], self.config['n_s'] = train_dataset.get_n_variables()
        self.model = NHITS(**self.config)

        early_stopping = EarlyStopping(monitor="val_loss",
                                       min_delta=1e-4,
                                       patience=self.config['early_stop_patience'],
                                       verbose=False,
                                       mode="min")

        trainer = pl.Trainer(max_epochs=self.config['max_epochs'],
                             max_steps=self.config['max_steps'],
                             gradient_clip_val=1.0,
                             progress_bar_refresh_rate=10,
                             log_every_n_steps=500,
                             check_val_every_n_epoch=1,
                             callbacks=[early_stopping])

        trainer.fit(self.model, train_loader, val_loader)

        # end reference

        # for group in self.ts_analysis['group_combinations']:
            # pass

    def partial_fit(self, train_data: EncodedDs, dev_data: EncodedDs) -> None:
        """
        Due to how lightwood implements the `update` procedure, expected inputs for this method are:
        
        :param dev_data: original `test` split (used to validate and select model if ensemble is `BestOf`).
        :param train_data: concatenated original `train` and `dev` splits.
        """  # noqa
        self.hyperparam_search = False
        self.fit(dev_data, train_data)
        self.prepared = True

    def __call__(self, ds: Union[EncodedDs, ConcatedEncodedDs],
                 args: PredictionArguments = PredictionArguments()) -> pd.DataFrame:
        """
        Calls the mixer to emit forecasts.
        """  # noqa
        if args.predict_proba:
            log.warning('This mixer does not output probability estimates')

        length = sum(ds.encoded_ds_lenghts) if isinstance(ds, ConcatedEncodedDs) else len(ds)
        ydf = pd.DataFrame(0,  # zero-filled
                           index=np.arange(length),
                           columns=['prediction'],
                           dtype=object)

        data = {'data': ds.data_frame[self.target],
                'group_info': {gcol: ds.data_frame[gcol].tolist()
                               for gcol in self.grouped_by} if self.ts_analysis['tss'].group_by else {}}

        pending_idxs = set(range(length))
        all_group_combinations = list(product(*[set(x) for x in data['group_info'].values()]))
        for group in all_group_combinations:
            series_idxs, series_data = get_group_matches(data, group)

            if series_data.size > 0:
                group = frozenset(group)
                series_idxs = sorted(series_idxs)
                if self.models.get(group, False) and self.models[group].is_fitted:
                    forecaster = self.models[group]
                else:
                    log.warning(f"Applying default forecaster for novel group {group}. Performance might not be optimal.")  # noqa
                    forecaster = self.models['__default']
                series = pd.Series(series_data.squeeze(), index=series_idxs)
                ydf = self._call_groupmodel(ydf, forecaster, series, offset=args.forecast_offset)
                pending_idxs -= set(series_idxs)

        # apply default model in all remaining novel-group rows
        if len(pending_idxs) > 0:
            series = pd.Series(data['data'][list(pending_idxs)].squeeze(), index=sorted(list(pending_idxs)))
            ydf = self._call_groupmodel(ydf, self.models['__default'], series, offset=args.forecast_offset)

        return ydf[['prediction']]

    def _call_groupmodel(self,
                         ydf: pd.DataFrame,
                         model,
                         series: pd.Series,
                         offset: int = 0):
        """
        Inner method that calls a `sktime.BaseForecaster`.

        :param offset: indicates relative offset to the latest data point seen during model training. Cannot be less than the number of training data points + the amount of diffences applied internally by the model.
        """  # noqa
        # original_index = series.index
        # series = series.reset_index(drop=True)
        # ydf['prediction'].iloc[original_index[idx]] = model.predict(np.arange(start_idx, end_idx)).tolist()

        return  # ydf
