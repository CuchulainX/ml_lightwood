{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we will go through an example to update a preexisting model. This might be useful when you come across additional data that you would want to consider, without having to train a model from scratch.\n",
    "\n",
    "The main abstraction that Lightwood offers for this is the `BaseMixer.partial_fit()` method. To call it, you need to pass new training data and a held-out dev subset for internal mixer usage (e.g. early stopping). If you are using an aggregate ensemble, it's likely you will want to do this for every single mixer. The convienient `PredictorInterface.adjust()` does this automatically for you.\n",
    "\n",
    "\n",
    "# Initial model training\n",
    "\n",
    "First, let's train a Lightwood predictor for the `concrete strength` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T23:11:20.600172Z",
     "iopub.status.busy": "2021-11-22T23:11:20.599846Z",
     "iopub.status.idle": "2021-11-22T23:11:22.119519Z",
     "shell.execute_reply": "2021-11-22T23:11:22.119214Z"
    }
   },
   "outputs": [],
   "source": [
    "from lightwood.api.high_level import ProblemDefinition, json_ai_from_problem, predictor_from_json_ai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T23:11:22.123971Z",
     "iopub.status.busy": "2021-11-22T23:11:22.123668Z",
     "iopub.status.idle": "2021-11-22T23:11:25.267183Z",
     "shell.execute_reply": "2021-11-22T23:11:25.267800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mindsdb/lightwood/staging/tests/data/concrete_strength.csv')\n",
    "\n",
    "df = df.sample(frac=1, random_state=1)\n",
    "train_df = df[:int(0.1*len(df))]\n",
    "update_df = df[int(0.1*len(df)):int(0.8*len(df))]\n",
    "test_df = df[int(0.8*len(df)):]\n",
    "\n",
    "print(f'Train dataframe shape: {train_df.shape}')\n",
    "print(f'Update dataframe shape: {update_df.shape}')\n",
    "print(f'Test dataframe shape: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have three different data splits.\n",
    "\n",
    "We will use the `training` split for the initial model training. As you can see, it's only a 20% of the total data we have. The `update` split will be used as training data to adjust/update our model. Finally, the held out `test` set will give us a rough idea of the impact our updating procedure has on the model's predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T23:11:25.276982Z",
     "iopub.status.busy": "2021-11-22T23:11:25.276207Z",
     "iopub.status.idle": "2021-11-22T23:11:26.721636Z",
     "shell.execute_reply": "2021-11-22T23:11:26.721360Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define predictive task and predictor\n",
    "target = 'concrete_strength'\n",
    "pdef = ProblemDefinition.from_dict({'target': target, 'time_aim': 200})\n",
    "jai = json_ai_from_problem(df, pdef)\n",
    "\n",
    "# We will keep the architecture simple: a single neural mixer, and a `BestOf` ensemble:\n",
    "jai.model = {\n",
    "    \"module\": \"BestOf\",\n",
    "    \"args\": {\n",
    "        \"args\": \"$pred_args\",\n",
    "        \"accuracy_functions\": \"$accuracy_functions\",\n",
    "        \"submodels\": [{\n",
    "            \"module\": \"Neural\",\n",
    "            \"args\": {\n",
    "                \"fit_on_dev\": False,\n",
    "                \"stop_after\": \"$problem_definition.seconds_per_mixer\",\n",
    "                \"search_hyperparameters\": False,\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Build and train the predictor\n",
    "predictor = predictor_from_json_ai(jai)\n",
    "predictor.learn(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T23:11:26.724576Z",
     "iopub.status.busy": "2021-11-22T23:11:26.724131Z",
     "iopub.status.idle": "2021-11-22T23:11:27.375479Z",
     "shell.execute_reply": "2021-11-22T23:11:27.375690Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train and get predictions for the held out test set\n",
    "predictions = predictor.predict(test_df)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the predictor\n",
    "\n",
    "For this, we have two options:\n",
    "\n",
    "### `BaseMixer.partial_fit()`\n",
    "\n",
    "Updates a single mixer. You need to pass the new data wrapped in `EncodedDs` objects.\n",
    "\n",
    "**Arguments:** \n",
    "* `train_data: EncodedDs`\n",
    "* `dev_data: EncodedDs`\n",
    "\n",
    "If the mixer does not need a `dev_data` partition, pass a dummy:\n",
    "\n",
    "```\n",
    "dev_data = EncodedDs(predictor.encoders, pd.DataFrame(), predictor.target)\n",
    "```\n",
    "\n",
    "### `PredictorInterface.adjust()`\n",
    "\n",
    "Updates all mixers inside the predictor by calling their respective `partial_fit()` methods.\n",
    "\n",
    "**Arguments:**\n",
    "* `new_data: Union[EncodedDs, ConcatedEncodedDs, pd.DataFrame]`\n",
    "* `old_data: Optional[Union[EncodedDs, ConcatedEncodedDs, pd.DataFrame]]`\n",
    "\n",
    "Let's `adjust` our predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T23:11:27.378492Z",
     "iopub.status.busy": "2021-11-22T23:11:27.378228Z",
     "iopub.status.idle": "2021-11-22T23:11:30.209384Z",
     "shell.execute_reply": "2021-11-22T23:11:30.209674Z"
    }
   },
   "outputs": [],
   "source": [
    "from lightwood.data import EncodedDs\n",
    "\n",
    "train_ds = EncodedDs(predictor.encoders, train_df, target)\n",
    "update_ds = EncodedDs(predictor.encoders, update_df, target)\n",
    "\n",
    "predictor.adjust(update_ds, train_ds)  # data to adjust and original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T23:11:30.212454Z",
     "iopub.status.busy": "2021-11-22T23:11:30.212012Z",
     "iopub.status.idle": "2021-11-22T23:11:30.919337Z",
     "shell.execute_reply": "2021-11-22T23:11:30.919563Z"
    }
   },
   "outputs": [],
   "source": [
    "new_predictions = predictor.predict(test_df)\n",
    "new_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Our predictor was updated, and new predictions are looking good. Let's compare the old and new accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T23:11:30.923430Z",
     "iopub.status.busy": "2021-11-22T23:11:30.923183Z",
     "iopub.status.idle": "2021-11-22T23:11:30.924850Z",
     "shell.execute_reply": "2021-11-22T23:11:30.924631Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "old_acc = r2_score(test_df['concrete_strength'], predictions['prediction'])\n",
    "new_acc = r2_score(test_df['concrete_strength'], new_predictions['prediction'])\n",
    "\n",
    "print(f'Old Accuracy: {round(old_acc, 3)}\\nNew Accuracy: {round(new_acc, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating, we see an increase in the R2 score of predictions for the held out test set.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have gone through a simple example of how Lightwood predictors can leverage newly acquired data to improve their predictions. The interface for doing so is fairly simple, requiring only some new data and a single call to update.\n",
    "\n",
    "You can further customize the logic for updating your mixers by modifying the `partial_fit()` methods in them."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
